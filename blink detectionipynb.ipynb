{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVoJpUCv6pV2",
        "outputId": "e3cf4c73-7dd0-4508-9277-721f4bf01168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.2)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python dlib imutils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MODEL_URL = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "\n",
        "# Download and extract the model\n",
        "!wget $MODEL_URL -O shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2  # Extract the file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B21H2qgL8qeU",
        "outputId": "91266b32-389b-4380-e99f-b989b77b8c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-23 21:17:53--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n",
            "--2025-02-23 21:17:53--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‚Äòshape_predictor_68_face_landmarks.dat.bz2‚Äô\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  38.9MB/s    in 1.6s    \n",
            "\n",
            "2025-02-23 21:17:55 (38.9 MB/s) - ‚Äòshape_predictor_68_face_landmarks.dat.bz2‚Äô saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# Load Dlib's face detector and the landmark predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "# Define a function to calculate the Eye Aspect Ratio (EAR)\n",
        "def eye_aspect_ratio(eye):\n",
        "    A = dist.euclidean(eye[1], eye[5])  # Vertical distance\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])  # Horizontal distance\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n"
      ],
      "metadata": {
        "id": "jGIAJaH38vSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define eye landmarks (indexes from 68-point model)\n",
        "(left_start, left_end) = (42, 48)\n",
        "(right_start, right_end) = (36, 42)\n",
        "\n",
        "EYE_AR_THRESH = 0.25  # EAR threshold for blink detection\n",
        "EYE_AR_CONSEC_FRAMES = 3  # Number of consecutive frames for a blink\n",
        "\n",
        "blink_counter = 0\n",
        "\n",
        "cap = cv2.VideoCapture(0)  # Open webcam\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
        "\n",
        "        left_eye = landmarks[left_start:left_end]\n",
        "        right_eye = landmarks[right_start:right_end]\n",
        "\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Draw eyes\n",
        "        cv2.polylines(frame, [left_eye], True, (0, 255, 0), 1)\n",
        "        cv2.polylines(frame, [right_eye], True, (0, 255, 0), 1)\n",
        "\n",
        "        # Check if EAR is below threshold\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            blink_counter += 1\n",
        "        else:\n",
        "            if blink_counter >= EYE_AR_CONSEC_FRAMES:\n",
        "                print(\"Blink detected!\")\n",
        "            blink_counter = 0\n",
        "\n",
        "    cv2.imshow(\"Blink Detection\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "w7D0EYTq8yb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def capture_webcam():\n",
        "    js_code = \"\"\"\n",
        "        async function captureWebcam() {\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'none';\n",
        "            document.body.appendChild(video);\n",
        "\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            return canvas.toDataURL('image/jpeg', 0.8);\n",
        "        }\n",
        "        captureWebcam();\n",
        "    \"\"\"\n",
        "    return eval_js(js_code)\n"
      ],
      "metadata": {
        "id": "fnT7fX1q9PXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_webcam_frame():\n",
        "    data_uri = capture_webcam()\n",
        "    image_bytes = b64decode(data_uri.split(',')[1])\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    nparr = np.frombuffer(image_bytes, np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect face\n",
        "    faces = detector(gray)\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
        "\n",
        "        left_eye = landmarks[left_start:left_end]\n",
        "        right_eye = landmarks[right_start:right_end]\n",
        "\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        print(f\"EAR: {ear:.2f}\")\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            print(\"Blink detected!\")\n",
        "\n",
        "# Run the function\n",
        "process_webcam_frame()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-JR9oH4r9SSO",
        "outputId": "a964e3fa-919e-4bc9-a396-e449766f7984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EAR: 0.15\n",
            "Blink detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Parameters for drowsiness detection\n",
        "DROWSINESS_THRESHOLD = 10  # Adjust based on testing\n",
        "CHECK_INTERVAL = 60  # Check every 60 seconds\n",
        "\n",
        "blink_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "def process_webcam_frame():\n",
        "    global blink_counter, start_time\n",
        "\n",
        "    data_uri = capture_webcam()\n",
        "    image_bytes = b64decode(data_uri.split(',')[1])\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    nparr = np.frombuffer(image_bytes, np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect face\n",
        "    faces = detector(gray)\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
        "\n",
        "        left_eye = landmarks[left_start:left_end]\n",
        "        right_eye = landmarks[right_start:right_end]\n",
        "\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        print(f\"EAR: {ear:.2f}\")\n",
        "\n",
        "        # Check if EAR is below threshold (blink detected)\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            blink_counter += 1\n",
        "            print(\"Blink detected!\")\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Check blink count every CHECK_INTERVAL seconds\n",
        "    if elapsed_time > CHECK_INTERVAL:\n",
        "        if blink_counter < DROWSINESS_THRESHOLD:\n",
        "            print(\"‚ö†Ô∏è Drowsiness Detected! Please take a break.\")\n",
        "\n",
        "        # Reset counters\n",
        "        blink_counter = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "# Run the function every time to capture a frame and check blinks\n",
        "process_webcam_frame()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "39DxIeIT9vq4",
        "outputId": "8c364558-5def-4d11-d371-2764a2833368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EAR: 0.15\n",
            "Blink detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import threading\n",
        "\n",
        "# Parameters for drowsiness detection\n",
        "DROWSINESS_THRESHOLD = 8  # Blinks per interval (adjust as needed)\n",
        "CHECK_INTERVAL = 30  # Time window in seconds\n",
        "FRAME_INTERVAL = 5  # Capture a frame every 5 seconds\n",
        "RUN_DURATION = 60  # Stop after 60 seconds\n",
        "\n",
        "blink_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "def process_webcam_frame():\n",
        "    global blink_counter, start_time\n",
        "\n",
        "    data_uri = capture_webcam()\n",
        "    image_bytes = b64decode(data_uri.split(',')[1])\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    nparr = np.frombuffer(image_bytes, np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect face\n",
        "    faces = detector(gray)\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
        "\n",
        "        left_eye = landmarks[left_start:left_end]\n",
        "        right_eye = landmarks[right_start:right_end]\n",
        "\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        print(f\"EAR: {ear:.2f}\")\n",
        "\n",
        "        # Check if EAR is below threshold (blink detected)\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            blink_counter += 1\n",
        "            print(\"Blink detected!\")\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Check blink count every CHECK_INTERVAL seconds\n",
        "    if elapsed_time > CHECK_INTERVAL:\n",
        "        print(f\"üîπ Blinks in {CHECK_INTERVAL} sec: {blink_counter}\")\n",
        "\n",
        "        if blink_counter < DROWSINESS_THRESHOLD:\n",
        "            print(\"‚ö†Ô∏è Drowsiness Detected! Please take a break.\")\n",
        "\n",
        "        # Reset counters\n",
        "        blink_counter = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "# Function to automate frame capturing every few seconds\n",
        "def start_drowsiness_detection():\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < RUN_DURATION:\n",
        "        process_webcam_frame()\n",
        "        time.sleep(FRAME_INTERVAL)  # Wait before capturing next frame\n",
        "\n",
        "# Start automatic detection\n",
        "start_drowsiness_detection()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "AxgaOAjD_Wj4",
        "outputId": "8b91e9e1-700f-471c-d79e-152e5c68ab7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EAR: 0.35\n",
            "EAR: 0.25\n",
            "EAR: 0.27\n",
            "EAR: 0.20\n",
            "Blink detected!\n",
            "EAR: 0.26\n",
            "EAR: 0.15\n",
            "Blink detected!\n",
            "üîπ Blinks in 30 sec: 2\n",
            "‚ö†Ô∏è Drowsiness Detected! Please take a break.\n",
            "EAR: 0.10\n",
            "Blink detected!\n",
            "EAR: 0.11\n",
            "Blink detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import dlib\n",
        "from scipy.spatial import distance\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# Load face detector and facial landmarks predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "# EAR threshold for blink detection\n",
        "EYE_AR_THRESH = 0.20\n",
        "\n",
        "# MAR threshold for yawn detection\n",
        "MOUTH_AR_THRESH = 0.7  # Increased threshold to reduce false yawns\n",
        "YAWN_CONSEC_FRAMES = 3  # Mouth must stay open for 3 frames to count as yawn\n",
        "yawn_frames = 0  # Counter for consecutive yawn frames\n",
        "\n",
        "# Parameters for drowsiness detection\n",
        "DROWSINESS_THRESHOLD = 8  # Blinks per interval\n",
        "CHECK_INTERVAL = 30  # Seconds\n",
        "FRAME_INTERVAL = 5  # Capture frame every 5 sec\n",
        "RUN_DURATION = 60  # Total runtime\n",
        "\n",
        "blink_counter = 0\n",
        "yawn_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# Function to compute EAR (Eye Aspect Ratio)\n",
        "def eye_aspect_ratio(eye):\n",
        "    A = distance.euclidean(eye[1], eye[5])\n",
        "    B = distance.euclidean(eye[2], eye[4])\n",
        "    C = distance.euclidean(eye[0], eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "# Function to compute MAR (Mouth Aspect Ratio)\n",
        "def mouth_aspect_ratio(mouth):\n",
        "    A = distance.euclidean(mouth[1], mouth[7])  # Vertical distance\n",
        "    B = distance.euclidean(mouth[2], mouth[6])\n",
        "    C = distance.euclidean(mouth[3], mouth[5])\n",
        "    D = distance.euclidean(mouth[0], mouth[4])  # Horizontal distance\n",
        "    return (A + B + C) / (3.0 * D)\n",
        "\n",
        "# Function to process webcam frames\n",
        "def process_webcam_frame():\n",
        "    global blink_counter, yawn_counter, yawn_frames, start_time\n",
        "\n",
        "    # Capture frame from webcam\n",
        "    data_uri = capture_webcam()\n",
        "    image_bytes = b64decode(data_uri.split(',')[1])\n",
        "    nparr = np.frombuffer(image_bytes, np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect face\n",
        "    faces = detector(gray)\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
        "\n",
        "        # Extract eye landmarks\n",
        "        left_eye = landmarks[36:42]\n",
        "        right_eye = landmarks[42:48]\n",
        "\n",
        "        # Extract mouth landmarks\n",
        "        mouth = landmarks[48:68]\n",
        "\n",
        "        # Calculate EAR (Eye Aspect Ratio)\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Calculate MAR (Mouth Aspect Ratio)\n",
        "        mar = mouth_aspect_ratio(mouth)\n",
        "\n",
        "        print(f\"EAR: {ear:.2f}, MAR: {mar:.2f}\")\n",
        "\n",
        "        # Detect Blink\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            blink_counter += 1\n",
        "            print(\"Blink detected!\")\n",
        "\n",
        "        # Detect Yawn only if the mouth stays open for consecutive frames\n",
        "        if mar > MOUTH_AR_THRESH:\n",
        "            yawn_frames += 1\n",
        "        else:\n",
        "            yawn_frames = 0  # Reset if mouth closes\n",
        "\n",
        "        if yawn_frames >= YAWN_CONSEC_FRAMES:\n",
        "            yawn_counter += 1\n",
        "            print(\"üòÆ Yawn detected!\")\n",
        "\n",
        "        # Draw landmarks for debugging\n",
        "        for (x, y) in mouth:\n",
        "            cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # Green dots for mouth landmarks\n",
        "\n",
        "    # Show frame with landmarks (for debugging)\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Check blink count every CHECK_INTERVAL seconds\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if elapsed_time > CHECK_INTERVAL:\n",
        "        print(f\"üîπ Blinks in {CHECK_INTERVAL} sec: {blink_counter}\")\n",
        "        print(f\"üîπ Yawns in {CHECK_INTERVAL} sec: {yawn_counter}\")\n",
        "\n",
        "        if blink_counter < DROWSINESS_THRESHOLD or yawn_counter >= YAWN_CONSEC_FRAMES:\n",
        "            print(\"‚ö†Ô∏è Drowsiness Detected! Please take a break.\")\n",
        "\n",
        "        # Reset counters\n",
        "        blink_counter = 0\n",
        "        yawn_counter = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "# Function to automate frame capturing\n",
        "def start_drowsiness_detection():\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < RUN_DURATION:\n",
        "        process_webcam_frame()\n",
        "        time.sleep(FRAME_INTERVAL)\n",
        "\n",
        "# Start automatic detection\n",
        "start_drowsiness_detection()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvBvkowoADin",
        "outputId": "1346b229-d0f2-49b9-d979-208ca79da777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'capture_webcam' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e8431be2ba24>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Start automatic detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mstart_drowsiness_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-e8431be2ba24>\u001b[0m in \u001b[0;36mstart_drowsiness_detection\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mRUN_DURATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mprocess_webcam_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAME_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e8431be2ba24>\u001b[0m in \u001b[0;36mprocess_webcam_frame\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Capture frame from webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdata_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapture_webcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mimage_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnparr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'capture_webcam' is not defined"
          ]
        }
      ]
    }
  ]
}